{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adatbővítésekkel CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSryvD_YGTYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import codecs, cv2, datetime, glob, itertools, keras, os, pickle\n",
        "import re, sklearn, string, sys, time\n",
        "import tensorflow as tf\n",
        "from random import randint\n",
        "from keras import backend as K, regularizers, optimizers\n",
        "from keras.models import load_model, Sequential\n",
        "from keras.layers import MaxPooling2D, Convolution2D, Activation, Dropout, Flatten, Dense, InputLayer\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc5vgJljvVVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shaping(data, target):\n",
        "    data = np.array(data, dtype=np.uint8)\n",
        "    target = np.array(target, dtype=np.uint8)\n",
        "    data = data.reshape(data.shape[0], img_height, img_width, channels)\n",
        "    target = np_utils.to_categorical(target, num_classes)\n",
        "    data = data.astype('float32')\n",
        "    data /= 255\n",
        "    return data, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lIpZ6KlOXGu",
        "colab_type": "code",
        "outputId": "d3ffde37-c049-403b-d55e-b5176be8f733",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2hPLXvYNwFn",
        "colab_type": "code",
        "outputId": "483d9c57-cc54-451e-f949-44374155198e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "#SVD-s adatokon\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(200)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(200)\n",
        "\n",
        "for j in range(25,26):\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "  \n",
        "  x_train_all = np.concatenate((x_train,x_test))\n",
        "  y_train_all = np.concatenate((y_train, y_test))\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x_train_all,y_train_all, test_size=10000, random_state=200, stratify=y_train_all)\n",
        "  \n",
        "  x_train_svded=[]\n",
        "  for i in range(X_train.shape[0]):\n",
        "    U, sigma, V = np.linalg.svd(X_train[i])\n",
        "    reconstimg = np.matrix(U[:, :j]) * np.diag(sigma[:j]) * np.matrix(V[:j, :])\n",
        "    x_train_svded.append(reconstimg)\n",
        "    \n",
        "  x_train_all = x_train_svded\n",
        "  img_width  = 28\n",
        "  img_height = 28\n",
        "  channels   = 1\n",
        "\n",
        "  batch_size = 250\n",
        "  num_epochs = 80\n",
        "  classes = {0: 'T-shirt/top',1: 'Trouser',2: 'Pullover',3: 'Dress',4: 'Coat',5: 'Sandal',6: 'Shirt',7: 'Sneaker',8: 'Bag',9: 'Ankle boot'}\n",
        "  num_classes         = len(classes)\n",
        "  classes_fashion     = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "  \n",
        "  cnn = Sequential()\n",
        "    \n",
        "  cnn.add(InputLayer(input_shape=(img_height,img_width,channels)))\n",
        "    \n",
        "  cnn.add(BatchNormalization())\n",
        "    \n",
        "  cnn.add(Convolution2D(64, (4, 4), padding='same', activation='relu'))\n",
        "  cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  cnn.add(Dropout(0.1))\n",
        "    \n",
        "  cnn.add(Convolution2D(64, (4, 4), activation='relu'))\n",
        "  cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  cnn.add(Dropout(0.3))\n",
        "\n",
        "  cnn.add(Flatten())\n",
        "\n",
        "  cnn.add(Dense(256, activation='relu'))\n",
        "  cnn.add(Dropout(0.5))\n",
        "    \n",
        "  cnn.add(Dense(64, activation='relu'))\n",
        "    \n",
        "  cnn.add(BatchNormalization())\n",
        "\n",
        "  cnn.add(Dense(num_classes, activation='softmax'))\n",
        "  cnn.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "  train_data_shaped, train_target_shaped  = shaping(x_train_all, y_train)\n",
        "  test_data_shaped, test_target_shaped    = shaping(X_test, y_test)\n",
        "  \n",
        "  histories = []\n",
        "  X_train, X_val, y_train, y_val = train_test_split(train_data_shaped, train_target_shaped, test_size=0.2, random_state=42)\n",
        "\n",
        "  history = cnn.fit(X_train,y_train,batch_size=batch_size,epochs=num_epochs,shuffle=False,verbose=0,validation_data=(X_val, y_val))\n",
        "    \n",
        "  histories.append(history.history)\n",
        "\n",
        "  predictions = cnn.predict_classes(test_data_shaped, verbose=0)\n",
        "  print('Matrix rank: ' + str(j) + ' Accuracy: ' + str(accuracy_score(y_test, predictions)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Matrix rank: 25 Accuracy: 0.9316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YybCETOb_AgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Eredeti + eredetiSVD-s\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(200)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(200)\n",
        "\n",
        "for j in range(25,26):\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "  \n",
        "  x_train_all = np.concatenate((x_train,x_test))\n",
        "  y_train_all = np.concatenate((y_train, y_test))\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x_train_all,y_train_all, test_size=10000, random_state=200, stratify=y_train_all)\n",
        "  \n",
        "  x_train_svded=[]\n",
        "  for i in range(X_train.shape[0]):\n",
        "    U, sigma, V = np.linalg.svd(X_train[i])\n",
        "    reconstimg = np.matrix(U[:, :j]) * np.diag(sigma[:j]) * np.matrix(V[:j, :])\n",
        "    x_train_svded.append(reconstimg)\n",
        "    \n",
        "  x_train_all = np.concatenate((X_train, x_train_svded))\n",
        "  y_train = np.concatenate((y_train, y_train))\n",
        "  \n",
        "  img_width  = 28\n",
        "  img_height = 28\n",
        "  channels   = 1\n",
        "\n",
        "  batch_size = 250\n",
        "  num_epochs = 80\n",
        "\n",
        "  cnn = Sequential()\n",
        "    \n",
        "  cnn.add(InputLayer(input_shape=(img_height,img_width,channels)))\n",
        "    \n",
        "  cnn.add(BatchNormalization())\n",
        "    \n",
        "  cnn.add(Convolution2D(64, (4, 4), padding='same', activation='relu'))\n",
        "  cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  cnn.add(Dropout(0.1))\n",
        "    \n",
        "  cnn.add(Convolution2D(64, (4, 4), activation='relu'))\n",
        "  cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  cnn.add(Dropout(0.3))\n",
        "\n",
        "  cnn.add(Flatten())\n",
        "\n",
        "  cnn.add(Dense(256, activation='relu'))\n",
        "  cnn.add(Dropout(0.5))\n",
        "    \n",
        "  cnn.add(Dense(64, activation='relu'))\n",
        "    \n",
        "  cnn.add(BatchNormalization())\n",
        "\n",
        "  cnn.add(Dense(10, activation='softmax'))\n",
        "  cnn.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "  train_data_shaped, train_target_shaped  = shaping(x_train_all, y_train)\n",
        "  test_data_shaped, test_target_shaped    = shaping(X_test, y_test)\n",
        "  \n",
        "  histories = []\n",
        "  X_train, X_val, y_train, y_val = train_test_split(train_data_shaped, train_target_shaped, test_size=0.2, random_state=42)\n",
        "\n",
        "  history = cnn.fit(X_train,y_train,batch_size=batch_size,epochs=num_epochs,shuffle=False,verbose=0,validation_data=(X_val, y_val))\n",
        "    \n",
        "  histories.append(history.history)\n",
        "\n",
        "  predictions = cnn.predict_classes(test_data_shaped, verbose=0)\n",
        "  print('Matrix rank: ' + str(j) + ' Accuracy: ' + str(accuracy_score(y_test, predictions)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m84-bxV-X8Qy",
        "colab_type": "code",
        "outputId": "061f0ce9-dd6d-4be4-da70-81ea27532bda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5090
        }
      },
      "source": [
        "#eredetin\n",
        "\n",
        "#from numpy.random import seed\n",
        "#seed(200)\n",
        "#from tensorflow import set_random_seed\n",
        "#set_random_seed(200)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "#(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "  \n",
        "#x_train_all = np.concatenate((x_train,x_test))\n",
        "#y_train_all = np.concatenate((y_train, y_test))\n",
        "#X_train, X_test, y_train, y_test = train_test_split(x_train_all,y_train_all, test_size=10000, random_state=200, stratify=y_train_all)\n",
        "\n",
        "#X_train = np.concatenate((X_train, X_train))\n",
        "#y_train = np.concatenate((y_train, y_train))\n",
        "\n",
        "img_width  = 28\n",
        "img_height = 28\n",
        "channels   = 1\n",
        "\n",
        "batch_size = 250\n",
        "num_epochs = 80\n",
        "\n",
        "cnn = Sequential()\n",
        "    \n",
        "cnn.add(InputLayer(input_shape=(img_height,img_width,channels)))\n",
        "    \n",
        "cnn.add(BatchNormalization())\n",
        "    \n",
        "cnn.add(Convolution2D(64, (4, 4), padding='same', activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(Dropout(0.1, seed=200))\n",
        "    \n",
        "cnn.add(Convolution2D(64, (4, 4), activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(Dropout(0.3, seed=200))\n",
        "\n",
        "cnn.add(Flatten())\n",
        "\n",
        "cnn.add(Dense(256, activation='relu'))\n",
        "cnn.add(Dropout(0.5, seed=200))\n",
        "    \n",
        "cnn.add(Dense(64, activation='relu'))\n",
        "    \n",
        "cnn.add(BatchNormalization())\n",
        "\n",
        "cnn.add(Dense(10, activation='softmax'))\n",
        "cnn.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "train_data_shaped, train_target_shaped  = shaping(X_train, y_train)\n",
        "test_data_shaped, test_target_shaped    = shaping(X_test, y_test)\n",
        "  \n",
        "histories = []\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_data_shaped, train_target_shaped, test_size=0.2, random_state=42)\n",
        "\n",
        "history = cnn.fit(X_train,y_train,batch_size=batch_size,epochs=num_epochs+70,shuffle=False,verbose=1,validation_data=(X_val, y_val))\n",
        "    \n",
        "histories.append(history.history)\n",
        "\n",
        "predictions = cnn.predict_classes(test_data_shaped, verbose=0)\n",
        "print('Accuracy: ' + str(accuracy_score(y_test, predictions)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 96000 samples, validate on 24000 samples\n",
            "Epoch 1/150\n",
            "96000/96000 [==============================] - 7s 78us/step - loss: 0.5370 - acc: 0.8066 - val_loss: 0.3127 - val_acc: 0.8887\n",
            "Epoch 2/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.3305 - acc: 0.8798 - val_loss: 0.2585 - val_acc: 0.9051\n",
            "Epoch 3/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.2896 - acc: 0.8938 - val_loss: 0.2321 - val_acc: 0.9150\n",
            "Epoch 4/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.2617 - acc: 0.9037 - val_loss: 0.2183 - val_acc: 0.9191\n",
            "Epoch 5/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.2414 - acc: 0.9114 - val_loss: 0.2079 - val_acc: 0.9219\n",
            "Epoch 6/150\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.2247 - acc: 0.9175 - val_loss: 0.1859 - val_acc: 0.9284\n",
            "Epoch 7/150\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.2132 - acc: 0.9211 - val_loss: 0.1833 - val_acc: 0.9305\n",
            "Epoch 8/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.2024 - acc: 0.9240 - val_loss: 0.1858 - val_acc: 0.9299\n",
            "Epoch 9/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.1919 - acc: 0.9283 - val_loss: 0.1730 - val_acc: 0.9363\n",
            "Epoch 10/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.1815 - acc: 0.9333 - val_loss: 0.1550 - val_acc: 0.9419\n",
            "Epoch 11/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.1761 - acc: 0.9342 - val_loss: 0.1555 - val_acc: 0.9426\n",
            "Epoch 12/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.1713 - acc: 0.9354 - val_loss: 0.1523 - val_acc: 0.9432\n",
            "Epoch 13/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.1630 - acc: 0.9391 - val_loss: 0.1459 - val_acc: 0.9459\n",
            "Epoch 14/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.1583 - acc: 0.9411 - val_loss: 0.1429 - val_acc: 0.9464\n",
            "Epoch 15/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.1530 - acc: 0.9423 - val_loss: 0.1342 - val_acc: 0.9517\n",
            "Epoch 16/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.1486 - acc: 0.9445 - val_loss: 0.1391 - val_acc: 0.9490\n",
            "Epoch 17/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.1423 - acc: 0.9464 - val_loss: 0.1298 - val_acc: 0.9526\n",
            "Epoch 18/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.1391 - acc: 0.9476 - val_loss: 0.1247 - val_acc: 0.9538\n",
            "Epoch 19/150\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.1364 - acc: 0.9494 - val_loss: 0.1233 - val_acc: 0.9552\n",
            "Epoch 20/150\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1318 - acc: 0.9506 - val_loss: 0.1181 - val_acc: 0.9576\n",
            "Epoch 21/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.1283 - acc: 0.9517 - val_loss: 0.1236 - val_acc: 0.9548\n",
            "Epoch 22/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.1283 - acc: 0.9514 - val_loss: 0.1188 - val_acc: 0.9565\n",
            "Epoch 23/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.1224 - acc: 0.9543 - val_loss: 0.1128 - val_acc: 0.9589\n",
            "Epoch 24/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.1221 - acc: 0.9543 - val_loss: 0.1083 - val_acc: 0.9604\n",
            "Epoch 25/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.1159 - acc: 0.9566 - val_loss: 0.1110 - val_acc: 0.9599\n",
            "Epoch 26/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.1138 - acc: 0.9573 - val_loss: 0.1027 - val_acc: 0.9644\n",
            "Epoch 27/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.1140 - acc: 0.9570 - val_loss: 0.1084 - val_acc: 0.9613\n",
            "Epoch 28/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.1083 - acc: 0.9594 - val_loss: 0.1022 - val_acc: 0.9629\n",
            "Epoch 29/150\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1061 - acc: 0.9609 - val_loss: 0.1025 - val_acc: 0.9644\n",
            "Epoch 30/150\n",
            "96000/96000 [==============================] - 7s 68us/step - loss: 0.1064 - acc: 0.9601 - val_loss: 0.1008 - val_acc: 0.9631\n",
            "Epoch 31/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.1050 - acc: 0.9615 - val_loss: 0.0997 - val_acc: 0.9647\n",
            "Epoch 32/150\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.1032 - acc: 0.9616 - val_loss: 0.1019 - val_acc: 0.9646\n",
            "Epoch 33/150\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1000 - acc: 0.9631 - val_loss: 0.0952 - val_acc: 0.9670\n",
            "Epoch 34/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0967 - acc: 0.9643 - val_loss: 0.0929 - val_acc: 0.9686\n",
            "Epoch 35/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0960 - acc: 0.9639 - val_loss: 0.0905 - val_acc: 0.9688\n",
            "Epoch 36/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0954 - acc: 0.9643 - val_loss: 0.0901 - val_acc: 0.9704\n",
            "Epoch 37/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0913 - acc: 0.9656 - val_loss: 0.1006 - val_acc: 0.9642\n",
            "Epoch 38/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0911 - acc: 0.9655 - val_loss: 0.0869 - val_acc: 0.9700\n",
            "Epoch 39/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0902 - acc: 0.9658 - val_loss: 0.0885 - val_acc: 0.9712\n",
            "Epoch 40/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0911 - acc: 0.9663 - val_loss: 0.0843 - val_acc: 0.9710\n",
            "Epoch 41/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0890 - acc: 0.9671 - val_loss: 0.0898 - val_acc: 0.9693\n",
            "Epoch 42/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0875 - acc: 0.9672 - val_loss: 0.0823 - val_acc: 0.9734\n",
            "Epoch 43/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0858 - acc: 0.9682 - val_loss: 0.0835 - val_acc: 0.9720\n",
            "Epoch 44/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0848 - acc: 0.9688 - val_loss: 0.0892 - val_acc: 0.9689\n",
            "Epoch 45/150\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.0832 - acc: 0.9691 - val_loss: 0.0813 - val_acc: 0.9730\n",
            "Epoch 46/150\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.0799 - acc: 0.9702 - val_loss: 0.0817 - val_acc: 0.9732\n",
            "Epoch 47/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0811 - acc: 0.9693 - val_loss: 0.0763 - val_acc: 0.9753\n",
            "Epoch 48/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0821 - acc: 0.9695 - val_loss: 0.0764 - val_acc: 0.9744\n",
            "Epoch 49/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0795 - acc: 0.9704 - val_loss: 0.0801 - val_acc: 0.9733\n",
            "Epoch 50/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0772 - acc: 0.9715 - val_loss: 0.0783 - val_acc: 0.9753\n",
            "Epoch 51/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0756 - acc: 0.9722 - val_loss: 0.0784 - val_acc: 0.9744\n",
            "Epoch 52/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0758 - acc: 0.9724 - val_loss: 0.0789 - val_acc: 0.9739\n",
            "Epoch 53/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0762 - acc: 0.9720 - val_loss: 0.0816 - val_acc: 0.9721\n",
            "Epoch 54/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0736 - acc: 0.9725 - val_loss: 0.0772 - val_acc: 0.9754\n",
            "Epoch 55/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0725 - acc: 0.9732 - val_loss: 0.0752 - val_acc: 0.9768\n",
            "Epoch 56/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0728 - acc: 0.9731 - val_loss: 0.0747 - val_acc: 0.9767\n",
            "Epoch 57/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0727 - acc: 0.9729 - val_loss: 0.0730 - val_acc: 0.9772\n",
            "Epoch 58/150\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.0717 - acc: 0.9734 - val_loss: 0.0757 - val_acc: 0.9768\n",
            "Epoch 59/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0724 - acc: 0.9738 - val_loss: 0.0789 - val_acc: 0.9750\n",
            "Epoch 60/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0704 - acc: 0.9746 - val_loss: 0.0728 - val_acc: 0.9770\n",
            "Epoch 61/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0695 - acc: 0.9741 - val_loss: 0.0712 - val_acc: 0.9777\n",
            "Epoch 62/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0713 - acc: 0.9734 - val_loss: 0.0734 - val_acc: 0.9767\n",
            "Epoch 63/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0687 - acc: 0.9752 - val_loss: 0.0705 - val_acc: 0.9785\n",
            "Epoch 64/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0688 - acc: 0.9744 - val_loss: 0.0705 - val_acc: 0.9786\n",
            "Epoch 65/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0686 - acc: 0.9751 - val_loss: 0.0718 - val_acc: 0.9779\n",
            "Epoch 66/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0654 - acc: 0.9760 - val_loss: 0.0690 - val_acc: 0.9794\n",
            "Epoch 67/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0662 - acc: 0.9755 - val_loss: 0.0770 - val_acc: 0.9760\n",
            "Epoch 68/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0663 - acc: 0.9763 - val_loss: 0.0727 - val_acc: 0.9782\n",
            "Epoch 69/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0648 - acc: 0.9761 - val_loss: 0.0715 - val_acc: 0.9782\n",
            "Epoch 70/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0634 - acc: 0.9769 - val_loss: 0.0680 - val_acc: 0.9802\n",
            "Epoch 71/150\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.0635 - acc: 0.9761 - val_loss: 0.0740 - val_acc: 0.9771\n",
            "Epoch 72/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0633 - acc: 0.9765 - val_loss: 0.0684 - val_acc: 0.9797\n",
            "Epoch 73/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0622 - acc: 0.9770 - val_loss: 0.0786 - val_acc: 0.9755\n",
            "Epoch 74/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0627 - acc: 0.9768 - val_loss: 0.0665 - val_acc: 0.9798\n",
            "Epoch 75/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0616 - acc: 0.9775 - val_loss: 0.0700 - val_acc: 0.9808\n",
            "Epoch 76/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0596 - acc: 0.9782 - val_loss: 0.0667 - val_acc: 0.9805\n",
            "Epoch 77/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0594 - acc: 0.9783 - val_loss: 0.0709 - val_acc: 0.9787\n",
            "Epoch 78/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0639 - acc: 0.9767 - val_loss: 0.0685 - val_acc: 0.9806\n",
            "Epoch 79/150\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.0610 - acc: 0.9777 - val_loss: 0.0654 - val_acc: 0.9816\n",
            "Epoch 80/150\n",
            "96000/96000 [==============================] - 7s 68us/step - loss: 0.0594 - acc: 0.9788 - val_loss: 0.0658 - val_acc: 0.9812\n",
            "Epoch 81/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0584 - acc: 0.9784 - val_loss: 0.0687 - val_acc: 0.9800\n",
            "Epoch 82/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0581 - acc: 0.9788 - val_loss: 0.0613 - val_acc: 0.9823\n",
            "Epoch 83/150\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.0579 - acc: 0.9788 - val_loss: 0.0660 - val_acc: 0.9814\n",
            "Epoch 84/150\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.0580 - acc: 0.9792 - val_loss: 0.0678 - val_acc: 0.9800\n",
            "Epoch 85/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0557 - acc: 0.9794 - val_loss: 0.0666 - val_acc: 0.9810\n",
            "Epoch 86/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0576 - acc: 0.9790 - val_loss: 0.0667 - val_acc: 0.9817\n",
            "Epoch 87/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0567 - acc: 0.9794 - val_loss: 0.0676 - val_acc: 0.9810\n",
            "Epoch 88/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0567 - acc: 0.9791 - val_loss: 0.0640 - val_acc: 0.9821\n",
            "Epoch 89/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0566 - acc: 0.9795 - val_loss: 0.0659 - val_acc: 0.9816\n",
            "Epoch 90/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0555 - acc: 0.9799 - val_loss: 0.0646 - val_acc: 0.9821\n",
            "Epoch 91/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0538 - acc: 0.9806 - val_loss: 0.0662 - val_acc: 0.9811\n",
            "Epoch 92/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0555 - acc: 0.9799 - val_loss: 0.0679 - val_acc: 0.9805\n",
            "Epoch 93/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0545 - acc: 0.9801 - val_loss: 0.0608 - val_acc: 0.9836\n",
            "Epoch 94/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0520 - acc: 0.9809 - val_loss: 0.0667 - val_acc: 0.9805\n",
            "Epoch 95/150\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.0533 - acc: 0.9809 - val_loss: 0.0644 - val_acc: 0.9817\n",
            "Epoch 96/150\n",
            "96000/96000 [==============================] - 6s 68us/step - loss: 0.0550 - acc: 0.9806 - val_loss: 0.0643 - val_acc: 0.9816\n",
            "Epoch 97/150\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.0536 - acc: 0.9805 - val_loss: 0.0600 - val_acc: 0.9831\n",
            "Epoch 98/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0541 - acc: 0.9806 - val_loss: 0.0648 - val_acc: 0.9813\n",
            "Epoch 99/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0513 - acc: 0.9812 - val_loss: 0.0599 - val_acc: 0.9835\n",
            "Epoch 100/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0522 - acc: 0.9811 - val_loss: 0.0661 - val_acc: 0.9820\n",
            "Epoch 101/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0517 - acc: 0.9813 - val_loss: 0.0634 - val_acc: 0.9835\n",
            "Epoch 102/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0519 - acc: 0.9809 - val_loss: 0.0617 - val_acc: 0.9829\n",
            "Epoch 103/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0534 - acc: 0.9809 - val_loss: 0.0640 - val_acc: 0.9834\n",
            "Epoch 104/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0511 - acc: 0.9813 - val_loss: 0.0688 - val_acc: 0.9807\n",
            "Epoch 105/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0512 - acc: 0.9815 - val_loss: 0.0657 - val_acc: 0.9827\n",
            "Epoch 106/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0513 - acc: 0.9814 - val_loss: 0.0640 - val_acc: 0.9834\n",
            "Epoch 107/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0511 - acc: 0.9818 - val_loss: 0.0623 - val_acc: 0.9834\n",
            "Epoch 108/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0497 - acc: 0.9823 - val_loss: 0.0667 - val_acc: 0.9822\n",
            "Epoch 109/150\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.0522 - acc: 0.9812 - val_loss: 0.0646 - val_acc: 0.9820\n",
            "Epoch 110/150\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.0486 - acc: 0.9825 - val_loss: 0.0634 - val_acc: 0.9823\n",
            "Epoch 111/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0482 - acc: 0.9828 - val_loss: 0.0626 - val_acc: 0.9833\n",
            "Epoch 112/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0507 - acc: 0.9819 - val_loss: 0.0624 - val_acc: 0.9834\n",
            "Epoch 113/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0490 - acc: 0.9822 - val_loss: 0.0625 - val_acc: 0.9833\n",
            "Epoch 114/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0482 - acc: 0.9823 - val_loss: 0.0616 - val_acc: 0.9838\n",
            "Epoch 115/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0478 - acc: 0.9823 - val_loss: 0.0623 - val_acc: 0.9831\n",
            "Epoch 116/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0490 - acc: 0.9823 - val_loss: 0.0626 - val_acc: 0.9836\n",
            "Epoch 117/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0485 - acc: 0.9825 - val_loss: 0.0598 - val_acc: 0.9852\n",
            "Epoch 118/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0488 - acc: 0.9826 - val_loss: 0.0632 - val_acc: 0.9842\n",
            "Epoch 119/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0457 - acc: 0.9830 - val_loss: 0.0662 - val_acc: 0.9825\n",
            "Epoch 120/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0470 - acc: 0.9832 - val_loss: 0.0615 - val_acc: 0.9842\n",
            "Epoch 121/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0467 - acc: 0.9832 - val_loss: 0.0598 - val_acc: 0.9852\n",
            "Epoch 122/150\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.0454 - acc: 0.9841 - val_loss: 0.0641 - val_acc: 0.9833\n",
            "Epoch 123/150\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.0459 - acc: 0.9834 - val_loss: 0.0609 - val_acc: 0.9845\n",
            "Epoch 124/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0469 - acc: 0.9831 - val_loss: 0.0676 - val_acc: 0.9823\n",
            "Epoch 125/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0459 - acc: 0.9836 - val_loss: 0.0691 - val_acc: 0.9819\n",
            "Epoch 126/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0466 - acc: 0.9833 - val_loss: 0.0622 - val_acc: 0.9838\n",
            "Epoch 127/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0469 - acc: 0.9831 - val_loss: 0.0592 - val_acc: 0.9849\n",
            "Epoch 128/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0435 - acc: 0.9841 - val_loss: 0.0621 - val_acc: 0.9840\n",
            "Epoch 129/150\n",
            "96000/96000 [==============================] - 7s 68us/step - loss: 0.0468 - acc: 0.9834 - val_loss: 0.0584 - val_acc: 0.9850\n",
            "Epoch 130/150\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.0449 - acc: 0.9840 - val_loss: 0.0590 - val_acc: 0.9848\n",
            "Epoch 131/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0444 - acc: 0.9839 - val_loss: 0.0589 - val_acc: 0.9849\n",
            "Epoch 132/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0455 - acc: 0.9838 - val_loss: 0.0603 - val_acc: 0.9850\n",
            "Epoch 133/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0458 - acc: 0.9837 - val_loss: 0.0592 - val_acc: 0.9849\n",
            "Epoch 134/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0434 - acc: 0.9843 - val_loss: 0.0578 - val_acc: 0.9852\n",
            "Epoch 135/150\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.0440 - acc: 0.9841 - val_loss: 0.0636 - val_acc: 0.9840\n",
            "Epoch 136/150\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.0443 - acc: 0.9839 - val_loss: 0.0629 - val_acc: 0.9842\n",
            "Epoch 137/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0434 - acc: 0.9845 - val_loss: 0.0589 - val_acc: 0.9859\n",
            "Epoch 138/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0437 - acc: 0.9843 - val_loss: 0.0627 - val_acc: 0.9830\n",
            "Epoch 139/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0429 - acc: 0.9844 - val_loss: 0.0595 - val_acc: 0.9851\n",
            "Epoch 140/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0437 - acc: 0.9846 - val_loss: 0.0621 - val_acc: 0.9836\n",
            "Epoch 141/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0432 - acc: 0.9843 - val_loss: 0.0584 - val_acc: 0.9851\n",
            "Epoch 142/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0450 - acc: 0.9838 - val_loss: 0.0580 - val_acc: 0.9853\n",
            "Epoch 143/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0423 - acc: 0.9849 - val_loss: 0.0618 - val_acc: 0.9848\n",
            "Epoch 144/150\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.0428 - acc: 0.9844 - val_loss: 0.0590 - val_acc: 0.9853\n",
            "Epoch 145/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0429 - acc: 0.9842 - val_loss: 0.0616 - val_acc: 0.9846\n",
            "Epoch 146/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0407 - acc: 0.9858 - val_loss: 0.0570 - val_acc: 0.9856\n",
            "Epoch 147/150\n",
            "96000/96000 [==============================] - 6s 64us/step - loss: 0.0444 - acc: 0.9837 - val_loss: 0.0600 - val_acc: 0.9851\n",
            "Epoch 148/150\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.0406 - acc: 0.9854 - val_loss: 0.0618 - val_acc: 0.9845\n",
            "Epoch 149/150\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.0411 - acc: 0.9854 - val_loss: 0.0572 - val_acc: 0.9855\n",
            "Epoch 150/150\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0408 - acc: 0.9852 - val_loss: 0.0594 - val_acc: 0.9847\n",
            "Accuracy: 0.9376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMGPcbR2X5jA",
        "colab_type": "code",
        "outputId": "de678c4b-3feb-4f1e-f2eb-c9c85d43794f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#eredeti + tükrözött + eredetiSVD-s + tükrözöttSVD-s\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(200)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(200)\n",
        "\n",
        "for j in range(20,21):\n",
        "  \n",
        "  \n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "  \n",
        "  x_train_all = np.concatenate((x_train,x_test))\n",
        "  y_train_all = np.concatenate((y_train, y_test))\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x_train_all,y_train_all, test_size=10000, random_state=200, stratify=y_train_all)\n",
        "  \n",
        "  x_train_flipped = []\n",
        "  for i in range(X_train.shape[0]):\n",
        "    x_train_flipped.append(np.flip(X_train[i],1))\n",
        "    \n",
        "  x_train_with_mirror = np.concatenate((X_train, x_train_flipped))\n",
        "  y_train = np.concatenate((y_train, y_train))\n",
        "  \n",
        "  x_train_svded=[]\n",
        "  for i in range(x_train_with_mirror.shape[0]):\n",
        "    U, sigma, V = np.linalg.svd(x_train_with_mirror[i])\n",
        "    reconstimg = np.matrix(U[:, :j]) * np.diag(sigma[:j]) * np.matrix(V[:j, :])\n",
        "    x_train_svded.append(reconstimg)\n",
        "    \n",
        "  x_train_all = np.concatenate((x_train_with_mirror, x_train_svded))\n",
        "  y_train = np.concatenate((y_train, y_train))\n",
        "  #x_train_all = x_train_svded\n",
        "  img_width  = 28\n",
        "  img_height = 28\n",
        "  channels   = 1\n",
        "\n",
        "  batch_size = 250\n",
        "  num_epochs = 80\n",
        "\n",
        "  cnn = Sequential()\n",
        "    \n",
        "  cnn.add(InputLayer(input_shape=(img_height,img_width,channels)))\n",
        "    \n",
        "  cnn.add(BatchNormalization())\n",
        "    \n",
        "  cnn.add(Convolution2D(64, (4, 4), padding='same', activation='relu'))\n",
        "  cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  cnn.add(Dropout(0.1))\n",
        "    \n",
        "  cnn.add(Convolution2D(64, (4, 4), activation='relu'))\n",
        "  cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  cnn.add(Dropout(0.3))\n",
        "\n",
        "  cnn.add(Flatten())\n",
        "\n",
        "  cnn.add(Dense(256, activation='relu'))\n",
        "  cnn.add(Dropout(0.5))\n",
        "    \n",
        "  cnn.add(Dense(64, activation='relu'))\n",
        "    \n",
        "  cnn.add(BatchNormalization())\n",
        "\n",
        "  cnn.add(Dense(10, activation='softmax'))\n",
        "  cnn.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "  train_data_shaped, train_target_shaped  = shaping(x_train_all, y_train)\n",
        "  test_data_shaped, test_target_shaped    = shaping(X_test, y_test)\n",
        "  \n",
        "  histories = []\n",
        "  X_train, X_val, y_train, y_val = train_test_split(train_data_shaped, train_target_shaped, test_size=0.2, random_state=42)\n",
        "\n",
        "  history = cnn.fit(X_train,y_train,batch_size=batch_size,epochs=num_epochs,shuffle=False,verbose=0,validation_data=(X_val, y_val))\n",
        " \n",
        "    \n",
        "  histories.append(history.history)\n",
        "\n",
        "  predictions = cnn.predict_classes(test_data_shaped, verbose=0)\n",
        "  print('Matrix rank: ' + str(j) + ' Accuracy: ' + str(accuracy_score(y_test, predictions)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matrix rank: 20 Accuracy: 0.9396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1OxP7jFvOr_",
        "colab_type": "code",
        "outputId": "9c5a3329-6579-4e43-9ad4-fede83e8a685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2738
        }
      },
      "source": [
        "#eredeti + tükrözött\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(7)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(7)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "  \n",
        "x_train_all = np.concatenate((x_train,x_test))\n",
        "y_train_all = np.concatenate((y_train, y_test))\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_train_all,y_train_all, test_size=10000, random_state=7, stratify=y_train_all)\n",
        "  \n",
        "x_train_flipped = []\n",
        "for i in range(X_train.shape[0]):\n",
        "  x_train_flipped.append(np.flip(X_train[i],1))\n",
        "    \n",
        "x_train_all = np.concatenate((X_train, x_train_flipped))\n",
        "y_train = np.concatenate((y_train, y_train))\n",
        "  \n",
        "img_width  = 28\n",
        "img_height = 28\n",
        "channels   = 1\n",
        "\n",
        "batch_size = 250\n",
        "num_epochs = 80\n",
        "\n",
        "cnn = Sequential()\n",
        "    \n",
        "cnn.add(InputLayer(input_shape=(img_height,img_width,channels)))\n",
        "    \n",
        "cnn.add(BatchNormalization())\n",
        "    \n",
        "cnn.add(Convolution2D(64, (4, 4), padding='same', activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(Dropout(0.1))\n",
        "    \n",
        "cnn.add(Convolution2D(64, (4, 4), activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(Dropout(0.3))\n",
        "\n",
        "cnn.add(Flatten())\n",
        "\n",
        "cnn.add(Dense(256, activation='relu'))\n",
        "cnn.add(Dropout(0.5))\n",
        "    \n",
        "cnn.add(Dense(64, activation='relu'))\n",
        "    \n",
        "cnn.add(BatchNormalization())\n",
        "\n",
        "cnn.add(Dense(10, activation='softmax'))\n",
        "cnn.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "train_data_shaped, train_target_shaped  = shaping(x_train_all, y_train)\n",
        "test_data_shaped, test_target_shaped    = shaping(X_test, y_test)\n",
        "  \n",
        "histories = []\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_data_shaped, train_target_shaped, test_size=0.2, random_state=42)\n",
        "\n",
        "history = cnn.fit(X_train,y_train,batch_size=batch_size,epochs=num_epochs,shuffle=False,verbose=1,validation_data=(X_val, y_val))\n",
        "    \n",
        "histories.append(history.history)\n",
        "\n",
        "predictions = cnn.predict_classes(test_data_shaped, verbose=0)\n",
        "print('Accuracy: ' + str(accuracy_score(y_test, predictions)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 96000 samples, validate on 24000 samples\n",
            "Epoch 1/80\n",
            "96000/96000 [==============================] - 8s 79us/step - loss: 0.5900 - acc: 0.7920 - val_loss: 0.3377 - val_acc: 0.8768\n",
            "Epoch 2/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.3578 - acc: 0.8711 - val_loss: 0.2871 - val_acc: 0.8942\n",
            "Epoch 3/80\n",
            "96000/96000 [==============================] - 7s 68us/step - loss: 0.3116 - acc: 0.8863 - val_loss: 0.2646 - val_acc: 0.9019\n",
            "Epoch 4/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.2890 - acc: 0.8933 - val_loss: 0.2404 - val_acc: 0.9105\n",
            "Epoch 5/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.2675 - acc: 0.9016 - val_loss: 0.2254 - val_acc: 0.9154\n",
            "Epoch 6/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.2491 - acc: 0.9082 - val_loss: 0.2248 - val_acc: 0.9153\n",
            "Epoch 7/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.2384 - acc: 0.9122 - val_loss: 0.2192 - val_acc: 0.9170\n",
            "Epoch 8/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.2278 - acc: 0.9159 - val_loss: 0.2046 - val_acc: 0.9230\n",
            "Epoch 9/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.2188 - acc: 0.9188 - val_loss: 0.2152 - val_acc: 0.9189\n",
            "Epoch 10/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.2116 - acc: 0.9213 - val_loss: 0.1954 - val_acc: 0.9286\n",
            "Epoch 11/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.2034 - acc: 0.9256 - val_loss: 0.1962 - val_acc: 0.9274\n",
            "Epoch 12/80\n",
            "96000/96000 [==============================] - 7s 69us/step - loss: 0.2000 - acc: 0.9255 - val_loss: 0.1912 - val_acc: 0.9277\n",
            "Epoch 13/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.1948 - acc: 0.9275 - val_loss: 0.1948 - val_acc: 0.9277\n",
            "Epoch 14/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.1890 - acc: 0.9297 - val_loss: 0.2041 - val_acc: 0.9234\n",
            "Epoch 15/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.1837 - acc: 0.9316 - val_loss: 0.1902 - val_acc: 0.9283\n",
            "Epoch 16/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1810 - acc: 0.9325 - val_loss: 0.1827 - val_acc: 0.9317\n",
            "Epoch 17/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1755 - acc: 0.9349 - val_loss: 0.1876 - val_acc: 0.9289\n",
            "Epoch 18/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1723 - acc: 0.9355 - val_loss: 0.1855 - val_acc: 0.9304\n",
            "Epoch 19/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1679 - acc: 0.9372 - val_loss: 0.1793 - val_acc: 0.9336\n",
            "Epoch 20/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1656 - acc: 0.9391 - val_loss: 0.1825 - val_acc: 0.9324\n",
            "Epoch 21/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.1638 - acc: 0.9392 - val_loss: 0.1793 - val_acc: 0.9340\n",
            "Epoch 22/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.1582 - acc: 0.9415 - val_loss: 0.1744 - val_acc: 0.9354\n",
            "Epoch 23/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.1571 - acc: 0.9409 - val_loss: 0.1737 - val_acc: 0.9362\n",
            "Epoch 24/80\n",
            "96000/96000 [==============================] - 6s 68us/step - loss: 0.1548 - acc: 0.9423 - val_loss: 0.1793 - val_acc: 0.9331\n",
            "Epoch 25/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.1540 - acc: 0.9422 - val_loss: 0.1750 - val_acc: 0.9356\n",
            "Epoch 26/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1470 - acc: 0.9452 - val_loss: 0.1667 - val_acc: 0.9381\n",
            "Epoch 27/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.1456 - acc: 0.9451 - val_loss: 0.1714 - val_acc: 0.9381\n",
            "Epoch 28/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.1425 - acc: 0.9469 - val_loss: 0.1789 - val_acc: 0.9361\n",
            "Epoch 29/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.1421 - acc: 0.9470 - val_loss: 0.1886 - val_acc: 0.9322\n",
            "Epoch 30/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1408 - acc: 0.9474 - val_loss: 0.1722 - val_acc: 0.9371\n",
            "Epoch 31/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1388 - acc: 0.9480 - val_loss: 0.1660 - val_acc: 0.9400\n",
            "Epoch 32/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1359 - acc: 0.9494 - val_loss: 0.1679 - val_acc: 0.9394\n",
            "Epoch 33/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.1346 - acc: 0.9494 - val_loss: 0.1849 - val_acc: 0.9325\n",
            "Epoch 34/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1338 - acc: 0.9498 - val_loss: 0.1734 - val_acc: 0.9382\n",
            "Epoch 35/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.1323 - acc: 0.9504 - val_loss: 0.1731 - val_acc: 0.9390\n",
            "Epoch 36/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1282 - acc: 0.9519 - val_loss: 0.1726 - val_acc: 0.9387\n",
            "Epoch 37/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1287 - acc: 0.9516 - val_loss: 0.1779 - val_acc: 0.9373\n",
            "Epoch 38/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1278 - acc: 0.9525 - val_loss: 0.1662 - val_acc: 0.9416\n",
            "Epoch 39/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1247 - acc: 0.9534 - val_loss: 0.1638 - val_acc: 0.9424\n",
            "Epoch 40/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1238 - acc: 0.9535 - val_loss: 0.1741 - val_acc: 0.9388\n",
            "Epoch 41/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1242 - acc: 0.9534 - val_loss: 0.1726 - val_acc: 0.9396\n",
            "Epoch 42/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1227 - acc: 0.9543 - val_loss: 0.1620 - val_acc: 0.9424\n",
            "Epoch 43/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1204 - acc: 0.9545 - val_loss: 0.1641 - val_acc: 0.9422\n",
            "Epoch 44/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1187 - acc: 0.9560 - val_loss: 0.1668 - val_acc: 0.9402\n",
            "Epoch 45/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1178 - acc: 0.9559 - val_loss: 0.1668 - val_acc: 0.9414\n",
            "Epoch 46/80\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.1164 - acc: 0.9567 - val_loss: 0.1663 - val_acc: 0.9421\n",
            "Epoch 47/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1174 - acc: 0.9565 - val_loss: 0.1610 - val_acc: 0.9436\n",
            "Epoch 48/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1133 - acc: 0.9578 - val_loss: 0.1739 - val_acc: 0.9390\n",
            "Epoch 49/80\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.1133 - acc: 0.9577 - val_loss: 0.1768 - val_acc: 0.9383\n",
            "Epoch 50/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1127 - acc: 0.9575 - val_loss: 0.1646 - val_acc: 0.9422\n",
            "Epoch 51/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1099 - acc: 0.9582 - val_loss: 0.1670 - val_acc: 0.9426\n",
            "Epoch 52/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1098 - acc: 0.9589 - val_loss: 0.1623 - val_acc: 0.9446\n",
            "Epoch 53/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1110 - acc: 0.9585 - val_loss: 0.1685 - val_acc: 0.9427\n",
            "Epoch 54/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1084 - acc: 0.9592 - val_loss: 0.1604 - val_acc: 0.9441\n",
            "Epoch 55/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1071 - acc: 0.9596 - val_loss: 0.1665 - val_acc: 0.9438\n",
            "Epoch 56/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1053 - acc: 0.9604 - val_loss: 0.1626 - val_acc: 0.9445\n",
            "Epoch 57/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1038 - acc: 0.9607 - val_loss: 0.1669 - val_acc: 0.9442\n",
            "Epoch 58/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1049 - acc: 0.9603 - val_loss: 0.1597 - val_acc: 0.9454\n",
            "Epoch 59/80\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.1025 - acc: 0.9614 - val_loss: 0.1654 - val_acc: 0.9439\n",
            "Epoch 60/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1016 - acc: 0.9623 - val_loss: 0.1602 - val_acc: 0.9447\n",
            "Epoch 61/80\n",
            "96000/96000 [==============================] - 6s 68us/step - loss: 0.1037 - acc: 0.9614 - val_loss: 0.1645 - val_acc: 0.9438\n",
            "Epoch 62/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1021 - acc: 0.9614 - val_loss: 0.1629 - val_acc: 0.9440\n",
            "Epoch 63/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.0999 - acc: 0.9630 - val_loss: 0.1651 - val_acc: 0.9441\n",
            "Epoch 64/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.1016 - acc: 0.9626 - val_loss: 0.1635 - val_acc: 0.9455\n",
            "Epoch 65/80\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.1000 - acc: 0.9619 - val_loss: 0.1597 - val_acc: 0.9448\n",
            "Epoch 66/80\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0989 - acc: 0.9637 - val_loss: 0.1674 - val_acc: 0.9444\n",
            "Epoch 67/80\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0990 - acc: 0.9633 - val_loss: 0.1665 - val_acc: 0.9438\n",
            "Epoch 68/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.0978 - acc: 0.9637 - val_loss: 0.1670 - val_acc: 0.9452\n",
            "Epoch 69/80\n",
            "96000/96000 [==============================] - 6s 65us/step - loss: 0.0988 - acc: 0.9629 - val_loss: 0.1601 - val_acc: 0.9452\n",
            "Epoch 70/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.0942 - acc: 0.9653 - val_loss: 0.1663 - val_acc: 0.9445\n",
            "Epoch 71/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.0962 - acc: 0.9640 - val_loss: 0.1697 - val_acc: 0.9439\n",
            "Epoch 72/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.0960 - acc: 0.9643 - val_loss: 0.1567 - val_acc: 0.9462\n",
            "Epoch 73/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.0943 - acc: 0.9653 - val_loss: 0.1639 - val_acc: 0.9452\n",
            "Epoch 74/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.0949 - acc: 0.9646 - val_loss: 0.1656 - val_acc: 0.9447\n",
            "Epoch 75/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.0915 - acc: 0.9662 - val_loss: 0.1594 - val_acc: 0.9451\n",
            "Epoch 76/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.0943 - acc: 0.9643 - val_loss: 0.1625 - val_acc: 0.9450\n",
            "Epoch 77/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.0925 - acc: 0.9655 - val_loss: 0.1611 - val_acc: 0.9442\n",
            "Epoch 78/80\n",
            "96000/96000 [==============================] - 6s 67us/step - loss: 0.0902 - acc: 0.9663 - val_loss: 0.1708 - val_acc: 0.9428\n",
            "Epoch 79/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.0935 - acc: 0.9655 - val_loss: 0.1621 - val_acc: 0.9460\n",
            "Epoch 80/80\n",
            "96000/96000 [==============================] - 6s 66us/step - loss: 0.0902 - acc: 0.9669 - val_loss: 0.1701 - val_acc: 0.9426\n",
            "Accuracy: 0.9313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHvojXK2Yw_D",
        "colab_type": "code",
        "outputId": "ca175103-528e-4dbf-f9cd-fe489c598890",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "#A bővített adathalmazok összehasonlítása:\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "eredeti=[0.9252,\t0.9301,\t0.9334,\t0.9256,\t0.9247]\n",
        "tukrozottek = [0.9296,\t0.9348,\t0.9344,\t0.9361,\t0.9313]\n",
        "svdsek = [[0.9248,\n",
        "0.9257,\n",
        "0.9246,\n",
        "0.9252,\n",
        "0.9275,\n",
        "0.9269,\n",
        "0.9233,\n",
        "0.9278,\n",
        "0.9296,\n",
        "0.9266,\n",
        "0.9293,\n",
        "0.9291,\n",
        "0.9273,\n",
        "0.9283,\n",
        "0.9277,\n",
        "0.9291,\n",
        "0.9339,\n",
        "0.9297,\n",
        "0.9301,\n",
        "0.9306,\n",
        "0.9292,\n",
        "0.9276\n",
        "],[0.9302,\n",
        "0.9313,\n",
        "0.93,\n",
        "0.9308,\n",
        "0.9291,\n",
        "0.9324,\n",
        "0.9349,\n",
        "0.9295,\n",
        "0.9347,\n",
        "0.9361,\n",
        "0.9346,\n",
        "0.9358,\n",
        "0.9345,\n",
        "0.9373,\n",
        "0.9314,\n",
        "0.9354,\n",
        "0.9364,\n",
        "0.9363,\n",
        "0.9388,\n",
        "0.9322,\n",
        "0.936,\n",
        "0.9387\n",
        "],[0.9309,\n",
        "0.9282,\n",
        "0.9328,\n",
        "0.9334,\n",
        "0.9313,\n",
        "0.9353,\n",
        "0.9338,\n",
        "0.9338,\n",
        "0.9336,\n",
        "0.9347,\n",
        "0.9349,\n",
        "0.9348,\n",
        "0.9338,\n",
        "0.9344,\n",
        "0.9346,\n",
        "0.9356,\n",
        "0.9351,\n",
        "0.9338,\n",
        "0.9357,\n",
        "0.9347,\n",
        "0.9367,\n",
        "0.9356\n",
        "],[0.9315,\n",
        "0.9264,\n",
        "0.927,\n",
        "0.9295,\n",
        "0.9339,\n",
        "0.9359,\n",
        "0.9337,\n",
        "0.9306,\n",
        "0.9339,\n",
        "0.934,\n",
        "0.9329,\n",
        "0.9318,\n",
        "0.9326,\n",
        "0.933,\n",
        "0.9357,\n",
        "0.935,\n",
        "0.9325,\n",
        "0.9349,\n",
        "0.9344,\n",
        "0.9301,\n",
        "0.9346,\n",
        "0.936\n",
        "],[0.9254,\n",
        "0.9261,\n",
        "0.9266,\n",
        "0.9275,\n",
        "0.927,\n",
        "0.9231,\n",
        "0.9293,\n",
        "0.9299,\n",
        "0.9271,\n",
        "0.9297,\n",
        "0.9293,\n",
        "0.9283,\n",
        "0.926,\n",
        "0.9327,\n",
        "0.9307,\n",
        "0.9313,\n",
        "0.9301,\n",
        "0.9329,\n",
        "0.9306,\n",
        "0.9315,\n",
        "0.9312,\n",
        "0.9359\n",
        "]]\n",
        "rangok = [4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]\n",
        "\n",
        "kombinalt = [[0.9259,\n",
        "0.9306,\n",
        "0.9284,\n",
        "0.9324,\n",
        "0.9308,\n",
        "0.9317,\n",
        "0.9332,\n",
        "0.9316,\n",
        "0.9298,\n",
        "0.9329,\n",
        "0.9337,\n",
        "0.932,\n",
        "0.9346,\n",
        "0.9336,\n",
        "0.9312,\n",
        "0.9325,\n",
        "0.9331\n",
        "],[0.9357,\n",
        "0.9367,\n",
        "0.9392,\n",
        "0.9382,\n",
        "0.9372,\n",
        "0.9367,\n",
        "0.9406,\n",
        "0.9391,\n",
        "0.9395,\n",
        "0.9399,\n",
        "0.9393,\n",
        "0.9399,\n",
        "0.9404,\n",
        "0.9426,\n",
        "0.9399,\n",
        "0.9402,\n",
        "0.9396\n",
        "],[0.9368,\n",
        "0.9349,\n",
        "0.9354,\n",
        "0.936,\n",
        "0.9357,\n",
        "0.9339,\n",
        "0.9411,\n",
        "0.9366,\n",
        "0.9413,\n",
        "0.9397,\n",
        "0.9371,\n",
        "0.939,\n",
        "0.9388,\n",
        "0.9389,\n",
        "0.9432,\n",
        "0.942,\n",
        "0.9402\n",
        "],[0.9338,\n",
        "0.9338,\n",
        "0.9348,\n",
        "0.9379,\n",
        "0.9389,\n",
        "0.9368,\n",
        "0.9384,\n",
        "0.937,\n",
        "0.9384,\n",
        "0.9378,\n",
        "0.935,\n",
        "0.938,\n",
        "0.9405,\n",
        "0.9389,\n",
        "0.9401,\n",
        "0.9387,\n",
        "0.9405\n",
        "],[0.9247,\n",
        "0.9297,\n",
        "0.9327,\n",
        "0.9321,\n",
        "0.935,\n",
        "0.9297,\n",
        "0.932,\n",
        "0.9331,\n",
        "0.9349,\n",
        "0.9362,\n",
        "0.9352,\n",
        "0.9373,\n",
        "0.9376,\n",
        "0.9363,\n",
        "0.9348,\n",
        "0.9355,\n",
        "0.9319\n",
        "]]\n",
        "rangok2=[4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
        "\n",
        "eredetiarray = np.empty(len(rangok))\n",
        "eredetiarray.fill(eredeti[0])\n",
        "tukorarray = np.empty(len(rangok))\n",
        "tukorarray.fill(tukrozottek[0])\n",
        "\n",
        "eredetiarray2 = np.empty(len(rangok))\n",
        "eredetiarray2.fill(eredeti[1])\n",
        "tukorarray2 = np.empty(len(rangok))\n",
        "tukorarray2.fill(tukrozottek[1])\n",
        "\n",
        "eredetiarray3 = np.empty(len(rangok))\n",
        "eredetiarray3.fill(eredeti[2])\n",
        "tukorarray3 = np.empty(len(rangok))\n",
        "tukorarray3.fill(tukrozottek[2])\n",
        "\n",
        "eredetiarray4 = np.empty(len(rangok))\n",
        "eredetiarray4.fill(eredeti[3])\n",
        "tukorarray4 = np.empty(len(rangok))\n",
        "tukorarray4.fill(tukrozottek[3])\n",
        "\n",
        "eredetiarray5 = np.empty(len(rangok))\n",
        "eredetiarray5.fill(eredeti[4])\n",
        "tukorarray5 = np.empty(len(rangok))\n",
        "tukorarray5.fill(tukrozottek[4])\n",
        "\n",
        "f, ax = plt.subplots(2, 2, sharey=False, figsize=(10,10))\n",
        "\n",
        "ax[0,0].plot(rangok, eredetiarray, 'r-')\n",
        "ax[0,0].plot(rangok, svdsek[0], 'bo')\n",
        "ax[0,0].plot(rangok, tukorarray, 'g-')\n",
        "ax[0,0].plot(rangok2, kombinalt[0], 'yo')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ax[0,1].plot(rangok, eredetiarray2, 'r-')\n",
        "ax[0,1].plot(rangok, svdsek[1], 'bo')\n",
        "ax[0,1].plot(rangok, tukorarray2, 'g-')\n",
        "ax[0,1].plot(rangok2, kombinalt[1], 'yo')\n",
        "\n",
        "\n",
        "\n",
        "ax[1,1].plot(rangok, eredetiarray4, 'r-')\n",
        "ax[1,1].plot(rangok, svdsek[3], 'bo')\n",
        "ax[1,1].plot(rangok, tukorarray4, 'g-')\n",
        "ax[1,1].plot(rangok2, kombinalt[3], 'yo')\n",
        "\n",
        "\n",
        "ax[1,0].plot(rangok, eredetiarray5, 'r-')\n",
        "ax[1,0].plot(rangok, svdsek[4], 'bo')\n",
        "ax[1,0].plot(rangok, tukorarray5, 'g-')\n",
        "ax[1,0].plot(rangok2, kombinalt[4], 'yo')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f24de361d68>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmMAAAJCCAYAAACMOMDuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x720 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}